
```markdown
# ğŸ“œ Persian Poets GPT-2 Fine-Tuning Experiments

This project contains my initial experiments fine-tuning **GPT-2** on Persian classical poetry, aiming to create a mini language model that mimics the style of famous poets like Ferdowsi, Saadi, and Hafez.

---

## ğŸ“ Project Structure

```

persian\_poets\_llms/
â”œâ”€â”€ experiments.ipynb       # Single notebook documenting fine-tuning and generation experiments
â”œâ”€â”€ datasets/               # Text files with poetry data and training samples
â””â”€â”€ results/                # Text files showing generated poetry outputs

````

---

## ğŸ”§ Technologies Used

- ğŸ¤– Hugging Face Transformers  
- ğŸ§  GPT-2 fine-tuning  
- ğŸ“ Jupyter Notebook for experimentation  
- ğŸ‡®ğŸ‡· Persian poetry datasets in UTF-8 encoding  

---

## ğŸš€ How to Run

### 1. Install Dependencies

```bash
pip install transformers datasets jupyter torch
````

### 2. Open Notebook

```bash
jupyter notebook experiments.ipynb
```

### 3. Run and Modify

* The notebook contains detailed cells for dataset loading, training, and text generation
* Feel free to tweak hyperparameters and try different training steps

---

## ğŸ¯ Purpose

* Document my first attempts at Persian poetry language modeling
* Experiment with small-scale fine-tuning of GPT-2
* Explore cultural language models in NLP

---

## ğŸ§ª Results

* Generated poetic text samples stored in the `results/` folder
* Some quality issues expected due to dataset size and training constraints

---

## ğŸ“œ License

MIT License â€” use and adapt as you wish.

---

## âœ‰ï¸ Contact

Created by [Pouya Soltani](https://github.com/pouyasolltani81)
Feedback and collaboration welcome!

```

If you want, I can help you create this as a file here or help you push it to GitHub!
```
